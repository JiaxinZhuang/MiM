# MiM
***

# 0. Requirements
* Python 3.8
* PyTorch 1.13
* MONAI 0.11
* CUDA 11.7
* cuDNN 8.5
* NVIDIA GPU with compute capability 8.6

```
pip install -r requirements.txt
```

## 1. Datasets
The details of the pretraining dataset and finetuning dataset are shown in the following figures. Since these datasets are all publicly available, you can download them from their official websites.

![Pretraining dataset](./assets/pretrained_dataset.png)

![Finetuning dataset](./assets/finetune_dataset.png)


The project contains two directories, _i.e.,_
1) Pretrain
2) Finetune


# 2. Pretrain
Please refer to the [Pretrain](./Pretrain) directory for the pretraining code. Make sure you have installed the requirements and downloaded the datasets.

```
bash scripts/xxx.sh
```


# 3. Finetune
Please refer to the [Finetune](./Finetune) directory for the finetuning code. Make sure you have installed the requirements and downloaded the datasets.

We provide the [pretrained model](https://hkustconnect-my.sharepoint.com/:f:/g/personal/jzhuangad_connect_ust_hk/ElCam2XpVflPvynd9Ymss44Bl1zeKf9gOt-YqsOhMKyY2g?e=fMdhl5
) via the Onedrive link and you can finetune the model. Let's take the BTCV as an example.

```
bash scripts/SwinBaseline/Covid1920_swinBaseline_fold0_231102.sh
```



## Reference
```
@article{zhuang2024mim,
  title={MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image Analysis},
  author={Zhuang, Jiaxin and Wu, Linshan and Wang, Qiong and Vardhanabhuti, Varut and Luo, Lin and Chen, Hao},
  journal={arXiv preprint arXiv:2404.15580},
  year={2024}
}
```

## Acknowledgement
This codebase heavily references the following projects and we thanks their efforts:

- [Masked Autoencoders](https://github.com/facebookresearch/mae)
- [SwinUNETR](https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR/Pretrain)

And wellcomes to star our repo!



