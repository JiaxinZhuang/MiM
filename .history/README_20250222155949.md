# MiM
***

# 0. Requirements
* Python 3.8
* PyTorch 1.13
* MONAI 0.11
* CUDA 11.7
* cuDNN 8.5
* NVIDIA GPU with compute capability 8.6

```
pip install -r requirements.txt
```

## 1. Datasets
The details of the pretraining dataset and finetuning dataset are shown in the following figures. Since these datasets are all publicly available, you can download them from their official websites.

![Pretraining dataset](./assets/pretrained_dataset.png)

![Finetuning dataset](./assets/finetune_dataset.png)


The project contains two directories, _i.e.,_
1) Pretrain
2) Finetune

We will update these codes soon.

# 2. Pretrain

```
cd Pretrain
bash scripts/xxx.sh
```


# 3. Finetune
We provide the pretrained model via the Onedrive link and you can finetune the model. Let's take the BTCV as an example.

```
cd Finetune
bash scripts/SwinBaseline/Covid1920_swinBaseline_fold0_231102.sh
```




## Reference
```
@article{zhuang2024mim,
  title={MiM: Mask in Mask Self-Supervised Pre-Training for 3D Medical Image Analysis},
  author={Zhuang, Jiaxin and Wu, Linshan and Wang, Qiong and Vardhanabhuti, Varut and Luo, Lin and Chen, Hao},
  journal={arXiv preprint arXiv:2404.15580},
  year={2024}
}
```

## Acknowledgement
This codebase heavily references the following projects and we thanks their efforts:

- [Masked Autoencoders](https://github.com/facebookresearch/mae)
- [SwinUNETR](https://github.com/Project-MONAI/research-contributions/tree/main/SwinUNETR/Pretrain)



